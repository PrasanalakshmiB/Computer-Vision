{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF_Pet_Classification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1STDTCZZW5g6"},"source":["# PROJECT: Pet Classification Tensorflow Model Using CNN"]},{"cell_type":"markdown","metadata":{"id":"bcjrxt5iKSt4"},"source":["# Project Objective\n","To build a CNN model that classifies the given pet images correctly into dog and cat images.\n","\n","The project scope document specifies the requirements for the project “Pet Classification Model Using CNN.” Apart from specifying the functional and nonfunctional requirements for the project, it also serves as an input for project scoping."]},{"cell_type":"markdown","metadata":{"id":"LDsKEqt4KZQ9"},"source":["# Project Description and Scope\n","We are provided with the following resources that can be used as inputs for your model:\n","\n","A collection of images of pets, that is​, ​cats and dogs. These images are of different sizes with varied lighting conditions.\n","Code template containing the following code blocks: a. Import modules (part 1) b. Set hyper parameters (part 2) c. Read image data set (part 3) d. Run TensorFlow model (part 4) You are expected to write the code for CNN image classification model (between Parts 3 and 4) using TensorFlow that trains on the data and calculates the accuracy score on the test data."]},{"cell_type":"markdown","metadata":{"id":"NguHQ6krKseW"},"source":["# Project Guidelines\n","Begin by extracting ipynb file and the data in the same folder. The CNN model (cnn_model_fn) should have the following layers: ● Input layer ● Convolutional layer 1 with 32 filters of kernel size[5,5] ● Pooling layer 1 with pool size**[2,2] **and stride 2 ● Convolutional layer 2 with 64 filters of kernel size[5,5] ● Pooling layer 2 with pool size[2,2] and stride 2 ● Dense layer whose output size is fixed in the hyper parameter: fc_size=32 ● Dropout layer with dropout probability 0.4 Predict the class by doing a softmax on the output of the dropout lay bold text\n","\n","This should be followed by training and evaluation: For the training step, define the loss function and minimize it ● For the evaluation step, calculate the accuracy Run the program for 100, 200, and 300 iterations, respectively. Follow this by a report on the final accuracy and loss on the evaluation data. Prerequisites To execute this project, refer to the installation guide in the downloads section of LMS."]},{"cell_type":"code","metadata":{"id":"o6NQX-5ZXj_X","outputId":"4c3e08a7-190f-46e7-8b25-63c11cc8e0fc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650012465035,"user_tz":-180,"elapsed":1720,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"8wONr1tOluHw","executionInfo":{"status":"ok","timestamp":1650012465035,"user_tz":-180,"elapsed":5,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}}},"source":["import cv2                 # working with, mainly resizing, images\n","import numpy as np         # dealing with arrays\n","import os                  # dealing with directories\n","from random import shuffle # mixing up or currently ordered data that might lead our network astray in training.\n","from tqdm import tqdm"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"__ZZSCZqlz6B","executionInfo":{"status":"ok","timestamp":1650012465036,"user_tz":-180,"elapsed":6,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}}},"source":["TRAIN_DIR = '/content/drive/MyDrive/PETDB/train'\n","TEST_DIR = '/content/drive/MyDrive/PETDB/test'\n","IMG_SIZE = 224\n","LR = 1e-3\n","\n","MODEL_NAME = 'pet_classifier_2_Conv_basic'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"RDPUw_7WnbaS","executionInfo":{"status":"ok","timestamp":1650012465036,"user_tz":-180,"elapsed":5,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}}},"source":["def label_img(folder_name):\n","    \n","    # conversion to one-hot array [cat,dog]\n","    #                            [much cat, no dog]\n","    if folder_name == 'cats': return [1,0]\n","    #                             [no cat, very doggo]\n","    elif folder_name == 'dogs': return [0,1]"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"LQovP4u-nvJp","executionInfo":{"status":"ok","timestamp":1650012465037,"user_tz":-180,"elapsed":6,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}}},"source":["def create_train_data():\n","    training_data = []\n","    for n,folder in enumerate(os.listdir(TRAIN_DIR)):\n","      images = os.listdir(os.path.join(TRAIN_DIR,folder))\n","      for i,image in enumerate(images):\n","        label = label_img(folder)\n","        path = os.path.join(TRAIN_DIR,folder,image)\n","        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n","        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n","        training_data.append([np.array(img),np.array(label)])\n","    shuffle(training_data)\n","    np.save('train_data.npy', training_data)\n","    return training_data"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"XI-0VoThqa5d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650012465603,"user_tz":-180,"elapsed":573,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}},"outputId":"e4a8f6d0-ba6a-40b6-dd0c-f6ef56c54f81"},"source":["X = create_train_data()"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  arr = np.asanyarray(arr)\n"]}]},{"cell_type":"code","metadata":{"id":"-N1UFwctqnqU","outputId":"f72629ef-d71e-45ed-f8f9-9e7d8ef2878f","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1650012465607,"user_tz":-180,"elapsed":23,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}}},"source":["os.getcwd()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Yn-mZZ_CqsX0","executionInfo":{"status":"ok","timestamp":1650012465607,"user_tz":-180,"elapsed":21,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}}},"source":["def create_test_data():\n","    test_data = []\n","    for n,folder in enumerate(os.listdir(TEST_DIR)):\n","      images = os.listdir(os.path.join(TEST_DIR,folder))\n","      for i,image in enumerate(images):\n","        label = label_img(folder)\n","        path = os.path.join(TEST_DIR,folder,image)\n","        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n","        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n","        img = img/255\n","        test_data.append([np.array(img),np.array(label)])\n","    shuffle(test_data)\n","    np.save('test_data.npy', test_data)\n","    return test_data"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5sKdmbWr7HX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650012465608,"user_tz":-180,"elapsed":22,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}},"outputId":"ba4d7846-8d84-43ad-c09c-eb81d814bb32"},"source":["Y = create_test_data()"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  arr = np.asanyarray(arr)\n"]}]},{"cell_type":"markdown","metadata":{"id":"f_E_lpLBQimk"},"source":["# Model Specification\n","The CNN model (cnn_model_fn) should have the following layers:\n","\n","*   Input layer \n","*  Convolutional layer 1 with 32 filters of kernel size[5,5]\n","* Pooling layer 1 with pool size**[2,2] **and stride 2\n","* Convolutional layer 2 with 64 filters of kernel size[5,5] \n","* Pooling layer 2 with pool size[2,2] and stride 2 \n","* Dense layer whose output size is fixed in the hyper parameter: fc_size=32\n","* Dropout layer with dropout probability 0.4 Predict the class by doing a softmax on the output of the dropout lay bold text\n","\n","\n"]},{"cell_type":"code","source":["!pip install tflearn "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6taKhuAVUdWi","executionInfo":{"status":"ok","timestamp":1650012470005,"user_tz":-180,"elapsed":4416,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}},"outputId":"13b4b665-e790-412f-a738-416abcbe2825"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tflearn in /usr/local/lib/python3.7/dist-packages (0.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.21.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"id":"fSPcLWXbsCXM","outputId":"e750aea4-af6b-4e74-8b05-60ebe965139c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650012475353,"user_tz":-180,"elapsed":5352,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}}},"source":["import tflearn\n","from tflearn.layers.conv import conv_2d, max_pool_2d\n","from tflearn.layers.core import input_data, dropout, fully_connected\n","from tflearn.layers.estimator import regression\n","\n","#Input layer\n","convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n","\n","#Convolutional layer 1 with 32 filters of kernel size[5,5]\n","convnet = conv_2d(convnet, 32, 5, activation='relu')\n","\n","#Pooling layer 1 with pool size**[2,2] **and stride 2\n","convnet = max_pool_2d(convnet, 2,strides = 2)\n","\n","#Convolutional layer 2 with 64 filters of kernel size[5,5]\n","convnet = conv_2d(convnet, 64, 5, activation='relu')\n","\n","#Pooling layer 2 with pool size[2,2] and stride 2\n","convnet = max_pool_2d(convnet, 2,strides = 2)\n","\n","#Dense layer whose output size is fixed in the hyper parameter: fc_size=32\n","convnet = fully_connected(convnet, 32, activation='relu')\n","\n","#Dropout layer with dropout probability 0.4 Predict the class by doing a softmax on the output of the dropout lay bold text\n","convnet = dropout(convnet, 0.4)\n","convnet = fully_connected(convnet, 2, activation='softmax')\n","convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n","model = tflearn.DNN(convnet, tensorboard_dir='log')\n"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tflearn/initializations.py:110: calling UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:548: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tflearn/initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"]}]},{"cell_type":"code","metadata":{"id":"cNiKjiNzuMYM","executionInfo":{"status":"ok","timestamp":1650012475354,"user_tz":-180,"elapsed":6,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}}},"source":["train = X\n","test = Y"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODcUJ-s-s5_i","executionInfo":{"status":"ok","timestamp":1650012475355,"user_tz":-180,"elapsed":6,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}}},"source":["X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n","Y = [i[1] for i in train]\n","\n","test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n","test_y = [i[1] for i in test]"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"s_JtM92HuSWq","outputId":"754ef039-3768-44f7-e1ba-1d5290731f0e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650012814589,"user_tz":-180,"elapsed":339240,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}}},"source":["history= model.fit({'input': X}, {'targets': Y}, n_epoch=300, validation_set=({'input': test_x}, {'targets': test_y}), \n","    snapshot_step=500, show_metric=True, run_id=MODEL_NAME)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------------------------\n","Run id: pet_classifier_2_Conv_basic\n","Log directory: log/\n","INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n","---------------------------------\n","Training samples: 40\n","Validation samples: 20\n","--\n","Training Step: 1  | time: 4.297s\n","| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 4.49477 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 2  | total loss: \u001b[1m\u001b[32m9.66164\u001b[0m\u001b[0m | time: 1.123s\n","| Adam | epoch: 002 | loss: 9.66164 - acc: 0.4500 | val_loss: 8.21560 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 3  | total loss: \u001b[1m\u001b[32m11.17633\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 003 | loss: 11.17633 - acc: 0.4909 | val_loss: 10.84616 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 4  | total loss: \u001b[1m\u001b[32m11.42878\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 004 | loss: 11.42878 - acc: 0.4977 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 5  | total loss: \u001b[1m\u001b[32m11.48703\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 005 | loss: 11.48703 - acc: 0.4993 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 6  | total loss: \u001b[1m\u001b[32m11.50368\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 006 | loss: 11.50368 - acc: 0.4998 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 7  | total loss: \u001b[1m\u001b[32m11.63867\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 007 | loss: 11.63867 - acc: 0.4849 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 8  | total loss: \u001b[1m\u001b[32m11.56794\u001b[0m\u001b[0m | time: 1.127s\n","| Adam | epoch: 008 | loss: 11.56794 - acc: 0.4934 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 9  | total loss: \u001b[1m\u001b[32m11.37811\u001b[0m\u001b[0m | time: 1.125s\n","| Adam | epoch: 009 | loss: 11.37811 - acc: 0.4969 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 10  | total loss: \u001b[1m\u001b[32m11.37811\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 010 | loss: 11.37811 - acc: 0.4984 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 11  | total loss: \u001b[1m\u001b[32m11.44197\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 011 | loss: 11.44197 - acc: 0.4992 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 12  | total loss: \u001b[1m\u001b[32m11.47390\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 012 | loss: 11.47390 - acc: 0.4996 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 13  | total loss: \u001b[1m\u001b[32m11.49063\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 013 | loss: 11.49063 - acc: 0.4997 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 14  | total loss: \u001b[1m\u001b[32m11.49975\u001b[0m\u001b[0m | time: 1.144s\n","| Adam | epoch: 014 | loss: 11.49975 - acc: 0.4998 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 15  | total loss: \u001b[1m\u001b[32m11.50490\u001b[0m\u001b[0m | time: 1.125s\n","| Adam | epoch: 015 | loss: 11.50490 - acc: 0.4999 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 16  | total loss: \u001b[1m\u001b[32m11.29856\u001b[0m\u001b[0m | time: 1.132s\n","| Adam | epoch: 016 | loss: 11.29856 - acc: 0.4999 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 17  | total loss: \u001b[1m\u001b[32m11.37573\u001b[0m\u001b[0m | time: 1.129s\n","| Adam | epoch: 017 | loss: 11.37573 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 18  | total loss: \u001b[1m\u001b[32m11.42322\u001b[0m\u001b[0m | time: 1.132s\n","| Adam | epoch: 018 | loss: 11.42322 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 19  | total loss: \u001b[1m\u001b[32m11.45312\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 019 | loss: 11.45312 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 20  | total loss: \u001b[1m\u001b[32m11.47235\u001b[0m\u001b[0m | time: 1.134s\n","| Adam | epoch: 020 | loss: 11.47235 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 21  | total loss: \u001b[1m\u001b[32m11.48494\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 021 | loss: 11.48494 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 22  | total loss: \u001b[1m\u001b[32m11.49334\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 022 | loss: 11.49334 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 23  | total loss: \u001b[1m\u001b[32m11.49902\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 023 | loss: 11.49902 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 24  | total loss: \u001b[1m\u001b[32m11.50293\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 024 | loss: 11.50293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 25  | total loss: \u001b[1m\u001b[32m11.66265\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 025 | loss: 11.66265 - acc: 0.4932 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 26  | total loss: \u001b[1m\u001b[32m11.62302\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 026 | loss: 11.62302 - acc: 0.4950 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 27  | total loss: \u001b[1m\u001b[32m11.59471\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 027 | loss: 11.59471 - acc: 0.4963 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 28  | total loss: \u001b[1m\u001b[32m11.57426\u001b[0m\u001b[0m | time: 1.144s\n","| Adam | epoch: 028 | loss: 11.57426 - acc: 0.4972 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 29  | total loss: \u001b[1m\u001b[32m11.55934\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 029 | loss: 11.55934 - acc: 0.4979 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 30  | total loss: \u001b[1m\u001b[32m11.54835\u001b[0m\u001b[0m | time: 1.123s\n","| Adam | epoch: 030 | loss: 11.54835 - acc: 0.4984 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 31  | total loss: \u001b[1m\u001b[32m11.54017\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 031 | loss: 11.54017 - acc: 0.4988 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 32  | total loss: \u001b[1m\u001b[32m11.53404\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 032 | loss: 11.53404 - acc: 0.4990 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 33  | total loss: \u001b[1m\u001b[32m11.52941\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 033 | loss: 11.52941 - acc: 0.4992 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 34  | total loss: \u001b[1m\u001b[32m11.52588\u001b[0m\u001b[0m | time: 1.132s\n","| Adam | epoch: 034 | loss: 11.52588 - acc: 0.4994 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 35  | total loss: \u001b[1m\u001b[32m11.52316\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 035 | loss: 11.52316 - acc: 0.4995 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 36  | total loss: \u001b[1m\u001b[32m11.52107\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 036 | loss: 11.52107 - acc: 0.4996 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 37  | total loss: \u001b[1m\u001b[32m11.63457\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 037 | loss: 11.63457 - acc: 0.4947 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 38  | total loss: \u001b[1m\u001b[32m11.61077\u001b[0m\u001b[0m | time: 1.123s\n","| Adam | epoch: 038 | loss: 11.61077 - acc: 0.4957 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 39  | total loss: \u001b[1m\u001b[32m11.59204\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 039 | loss: 11.59204 - acc: 0.4966 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 40  | total loss: \u001b[1m\u001b[32m11.57720\u001b[0m\u001b[0m | time: 1.121s\n","| Adam | epoch: 040 | loss: 11.57720 - acc: 0.4972 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 41  | total loss: \u001b[1m\u001b[32m11.67113\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 041 | loss: 11.67113 - acc: 0.4931 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 42  | total loss: \u001b[1m\u001b[32m11.64265\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 042 | loss: 11.64265 - acc: 0.4944 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 43  | total loss: \u001b[1m\u001b[32m11.61976\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 043 | loss: 11.61976 - acc: 0.4954 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 44  | total loss: \u001b[1m\u001b[32m11.60127\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 044 | loss: 11.60127 - acc: 0.4962 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 45  | total loss: \u001b[1m\u001b[32m11.50396\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 045 | loss: 11.50396 - acc: 0.4968 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 46  | total loss: \u001b[1m\u001b[32m11.50546\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 046 | loss: 11.50546 - acc: 0.4973 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 47  | total loss: \u001b[1m\u001b[32m11.50668\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 047 | loss: 11.50668 - acc: 0.4978 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 48  | total loss: \u001b[1m\u001b[32m11.50768\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 048 | loss: 11.50768 - acc: 0.4981 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 49  | total loss: \u001b[1m\u001b[32m11.50851\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 049 | loss: 11.50851 - acc: 0.4984 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 50  | total loss: \u001b[1m\u001b[32m11.50920\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 050 | loss: 11.50920 - acc: 0.4987 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 51  | total loss: \u001b[1m\u001b[32m11.50977\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 051 | loss: 11.50977 - acc: 0.4989 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 52  | total loss: \u001b[1m\u001b[32m11.51024\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 052 | loss: 11.51024 - acc: 0.4990 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 53  | total loss: \u001b[1m\u001b[32m11.51064\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 053 | loss: 11.51064 - acc: 0.4992 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 54  | total loss: \u001b[1m\u001b[32m11.51097\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 054 | loss: 11.51097 - acc: 0.4993 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 55  | total loss: \u001b[1m\u001b[32m11.51125\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 055 | loss: 11.51125 - acc: 0.4994 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 56  | total loss: \u001b[1m\u001b[32m11.51148\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 056 | loss: 11.51148 - acc: 0.4995 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 57  | total loss: \u001b[1m\u001b[32m11.51168\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 057 | loss: 11.51168 - acc: 0.4996 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 58  | total loss: \u001b[1m\u001b[32m11.51185\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 058 | loss: 11.51185 - acc: 0.4996 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 59  | total loss: \u001b[1m\u001b[32m11.51200\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 059 | loss: 11.51200 - acc: 0.4997 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 60  | total loss: \u001b[1m\u001b[32m11.51212\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 060 | loss: 11.51212 - acc: 0.4997 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 61  | total loss: \u001b[1m\u001b[32m11.51450\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 061 | loss: 11.51450 - acc: 0.4965 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 62  | total loss: \u001b[1m\u001b[32m11.51430\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 062 | loss: 11.51430 - acc: 0.4969 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 63  | total loss: \u001b[1m\u001b[32m11.51412\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 063 | loss: 11.51412 - acc: 0.4973 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 64  | total loss: \u001b[1m\u001b[32m11.51397\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 064 | loss: 11.51397 - acc: 0.4977 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 65  | total loss: \u001b[1m\u001b[32m11.51384\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 065 | loss: 11.51384 - acc: 0.4980 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 66  | total loss: \u001b[1m\u001b[32m11.44582\u001b[0m\u001b[0m | time: 1.121s\n","| Adam | epoch: 066 | loss: 11.44582 - acc: 0.5012 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 67  | total loss: \u001b[1m\u001b[32m11.45387\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 067 | loss: 11.45387 - acc: 0.5011 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 68  | total loss: \u001b[1m\u001b[32m11.46087\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 068 | loss: 11.46087 - acc: 0.5010 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 69  | total loss: \u001b[1m\u001b[32m11.46695\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 069 | loss: 11.46695 - acc: 0.5009 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 70  | total loss: \u001b[1m\u001b[32m11.47226\u001b[0m\u001b[0m | time: 1.123s\n","| Adam | epoch: 070 | loss: 11.47226 - acc: 0.5008 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 71  | total loss: \u001b[1m\u001b[32m11.47689\u001b[0m\u001b[0m | time: 1.110s\n","| Adam | epoch: 071 | loss: 11.47689 - acc: 0.5007 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 72  | total loss: \u001b[1m\u001b[32m11.48094\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 072 | loss: 11.48094 - acc: 0.5006 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 73  | total loss: \u001b[1m\u001b[32m11.48450\u001b[0m\u001b[0m | time: 1.121s\n","| Adam | epoch: 073 | loss: 11.48450 - acc: 0.5005 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 74  | total loss: \u001b[1m\u001b[32m11.48762\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 074 | loss: 11.48762 - acc: 0.5005 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 75  | total loss: \u001b[1m\u001b[32m11.49036\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 075 | loss: 11.49036 - acc: 0.5004 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 76  | total loss: \u001b[1m\u001b[32m11.49278\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 076 | loss: 11.49278 - acc: 0.5004 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 77  | total loss: \u001b[1m\u001b[32m11.49491\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 077 | loss: 11.49491 - acc: 0.5003 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 78  | total loss: \u001b[1m\u001b[32m11.49680\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 078 | loss: 11.49680 - acc: 0.5003 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 79  | total loss: \u001b[1m\u001b[32m11.49847\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 079 | loss: 11.49847 - acc: 0.5003 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 80  | total loss: \u001b[1m\u001b[32m11.50173\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 080 | loss: 11.50173 - acc: 0.4977 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 81  | total loss: \u001b[1m\u001b[32m11.50286\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 081 | loss: 11.50286 - acc: 0.4979 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 82  | total loss: \u001b[1m\u001b[32m11.50387\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 082 | loss: 11.50387 - acc: 0.4981 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 83  | total loss: \u001b[1m\u001b[32m11.50477\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 083 | loss: 11.50477 - acc: 0.4983 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 84  | total loss: \u001b[1m\u001b[32m11.50559\u001b[0m\u001b[0m | time: 1.123s\n","| Adam | epoch: 084 | loss: 11.50559 - acc: 0.4985 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 85  | total loss: \u001b[1m\u001b[32m11.50632\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 085 | loss: 11.50632 - acc: 0.4986 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 86  | total loss: \u001b[1m\u001b[32m11.50698\u001b[0m\u001b[0m | time: 1.121s\n","| Adam | epoch: 086 | loss: 11.50698 - acc: 0.4988 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 87  | total loss: \u001b[1m\u001b[32m11.50758\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 087 | loss: 11.50758 - acc: 0.4989 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 88  | total loss: \u001b[1m\u001b[32m11.50811\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 088 | loss: 11.50811 - acc: 0.4990 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 89  | total loss: \u001b[1m\u001b[32m11.50859\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 089 | loss: 11.50859 - acc: 0.4991 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 90  | total loss: \u001b[1m\u001b[32m11.50903\u001b[0m\u001b[0m | time: 1.110s\n","| Adam | epoch: 090 | loss: 11.50903 - acc: 0.4992 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 91  | total loss: \u001b[1m\u001b[32m11.50942\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 091 | loss: 11.50942 - acc: 0.4993 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 92  | total loss: \u001b[1m\u001b[32m11.50977\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 092 | loss: 11.50977 - acc: 0.4993 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 93  | total loss: \u001b[1m\u001b[32m11.51008\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 093 | loss: 11.51008 - acc: 0.4994 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 94  | total loss: \u001b[1m\u001b[32m11.51037\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 094 | loss: 11.51037 - acc: 0.4995 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 95  | total loss: \u001b[1m\u001b[32m11.51062\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 095 | loss: 11.51062 - acc: 0.4995 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 96  | total loss: \u001b[1m\u001b[32m11.51085\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 096 | loss: 11.51085 - acc: 0.4996 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 97  | total loss: \u001b[1m\u001b[32m11.51106\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 097 | loss: 11.51106 - acc: 0.4996 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 98  | total loss: \u001b[1m\u001b[32m11.51125\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 098 | loss: 11.51125 - acc: 0.4997 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 99  | total loss: \u001b[1m\u001b[32m11.45385\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 099 | loss: 11.45385 - acc: 0.5022 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 100  | total loss: \u001b[1m\u001b[32m11.45976\u001b[0m\u001b[0m | time: 1.110s\n","| Adam | epoch: 100 | loss: 11.45976 - acc: 0.5020 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 101  | total loss: \u001b[1m\u001b[32m11.46507\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 101 | loss: 11.46507 - acc: 0.5018 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 102  | total loss: \u001b[1m\u001b[32m11.46986\u001b[0m\u001b[0m | time: 1.121s\n","| Adam | epoch: 102 | loss: 11.46986 - acc: 0.5016 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 103  | total loss: \u001b[1m\u001b[32m11.47417\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 103 | loss: 11.47417 - acc: 0.5014 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 104  | total loss: \u001b[1m\u001b[32m11.47804\u001b[0m\u001b[0m | time: 1.124s\n","| Adam | epoch: 104 | loss: 11.47804 - acc: 0.5013 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 105  | total loss: \u001b[1m\u001b[32m11.48153\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 105 | loss: 11.48153 - acc: 0.5012 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 106  | total loss: \u001b[1m\u001b[32m11.48467\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 106 | loss: 11.48467 - acc: 0.5010 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 107  | total loss: \u001b[1m\u001b[32m11.48750\u001b[0m\u001b[0m | time: 1.121s\n","| Adam | epoch: 107 | loss: 11.48750 - acc: 0.5009 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 108  | total loss: \u001b[1m\u001b[32m11.49004\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 108 | loss: 11.49004 - acc: 0.5008 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 109  | total loss: \u001b[1m\u001b[32m11.49233\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 109 | loss: 11.49233 - acc: 0.5008 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 110  | total loss: \u001b[1m\u001b[32m11.49439\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 110 | loss: 11.49439 - acc: 0.5007 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 111  | total loss: \u001b[1m\u001b[32m11.49624\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 111 | loss: 11.49624 - acc: 0.5006 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 112  | total loss: \u001b[1m\u001b[32m11.49791\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 112 | loss: 11.49791 - acc: 0.5006 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 113  | total loss: \u001b[1m\u001b[32m11.49941\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 113 | loss: 11.49941 - acc: 0.5005 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 114  | total loss: \u001b[1m\u001b[32m11.44493\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 114 | loss: 11.44493 - acc: 0.5030 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 115  | total loss: \u001b[1m\u001b[32m11.45173\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 115 | loss: 11.45173 - acc: 0.5027 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 116  | total loss: \u001b[1m\u001b[32m11.45785\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 116 | loss: 11.45785 - acc: 0.5024 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 117  | total loss: \u001b[1m\u001b[32m11.46336\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 117 | loss: 11.46336 - acc: 0.5022 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 118  | total loss: \u001b[1m\u001b[32m11.46832\u001b[0m\u001b[0m | time: 1.129s\n","| Adam | epoch: 118 | loss: 11.46832 - acc: 0.5019 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 119  | total loss: \u001b[1m\u001b[32m11.47278\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 119 | loss: 11.47278 - acc: 0.5017 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 120  | total loss: \u001b[1m\u001b[32m11.47679\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 120 | loss: 11.47679 - acc: 0.5016 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 121  | total loss: \u001b[1m\u001b[32m11.48041\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 121 | loss: 11.48041 - acc: 0.5014 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 122  | total loss: \u001b[1m\u001b[32m11.48366\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 122 | loss: 11.48366 - acc: 0.5013 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 123  | total loss: \u001b[1m\u001b[32m11.48658\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 123 | loss: 11.48658 - acc: 0.5011 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 124  | total loss: \u001b[1m\u001b[32m11.48922\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 124 | loss: 11.48922 - acc: 0.5010 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 125  | total loss: \u001b[1m\u001b[32m11.49159\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 125 | loss: 11.49159 - acc: 0.5009 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 126  | total loss: \u001b[1m\u001b[32m11.43616\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 126 | loss: 11.43616 - acc: 0.5033 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 127  | total loss: \u001b[1m\u001b[32m11.44384\u001b[0m\u001b[0m | time: 1.134s\n","| Adam | epoch: 127 | loss: 11.44384 - acc: 0.5030 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 128  | total loss: \u001b[1m\u001b[32m11.45074\u001b[0m\u001b[0m | time: 1.123s\n","| Adam | epoch: 128 | loss: 11.45074 - acc: 0.5027 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 129  | total loss: \u001b[1m\u001b[32m11.39940\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 129 | loss: 11.39940 - acc: 0.5049 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 130  | total loss: \u001b[1m\u001b[32m11.41075\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 130 | loss: 11.41075 - acc: 0.5044 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 131  | total loss: \u001b[1m\u001b[32m11.42097\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 131 | loss: 11.42097 - acc: 0.5040 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 132  | total loss: \u001b[1m\u001b[32m11.43016\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 132 | loss: 11.43016 - acc: 0.5036 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 133  | total loss: \u001b[1m\u001b[32m11.49601\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 133 | loss: 11.49601 - acc: 0.5007 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 134  | total loss: \u001b[1m\u001b[32m11.49770\u001b[0m\u001b[0m | time: 1.110s\n","| Adam | epoch: 134 | loss: 11.49770 - acc: 0.5007 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 135  | total loss: \u001b[1m\u001b[32m11.49922\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 135 | loss: 11.49922 - acc: 0.5006 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 136  | total loss: \u001b[1m\u001b[32m11.44475\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 136 | loss: 11.44475 - acc: 0.5030 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 137  | total loss: \u001b[1m\u001b[32m11.39400\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 137 | loss: 11.39400 - acc: 0.5052 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 138  | total loss: \u001b[1m\u001b[32m11.40589\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 138 | loss: 11.40589 - acc: 0.5047 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 139  | total loss: \u001b[1m\u001b[32m11.41660\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 139 | loss: 11.41660 - acc: 0.5042 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 140  | total loss: \u001b[1m\u001b[32m11.42623\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 140 | loss: 11.42623 - acc: 0.5038 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 141  | total loss: \u001b[1m\u001b[32m11.43490\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 141 | loss: 11.43490 - acc: 0.5034 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 142  | total loss: \u001b[1m\u001b[32m11.44270\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 142 | loss: 11.44270 - acc: 0.5031 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 143  | total loss: \u001b[1m\u001b[32m11.44973\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 143 | loss: 11.44973 - acc: 0.5028 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 144  | total loss: \u001b[1m\u001b[32m11.45605\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 144 | loss: 11.45605 - acc: 0.5025 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 145  | total loss: \u001b[1m\u001b[32m11.46173\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 145 | loss: 11.46173 - acc: 0.5023 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 146  | total loss: \u001b[1m\u001b[32m11.46685\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 146 | loss: 11.46685 - acc: 0.5020 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 147  | total loss: \u001b[1m\u001b[32m11.47146\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 147 | loss: 11.47146 - acc: 0.5018 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 148  | total loss: \u001b[1m\u001b[32m11.47561\u001b[0m\u001b[0m | time: 1.121s\n","| Adam | epoch: 148 | loss: 11.47561 - acc: 0.5016 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 149  | total loss: \u001b[1m\u001b[32m11.47934\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 149 | loss: 11.47934 - acc: 0.5015 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 150  | total loss: \u001b[1m\u001b[32m11.48270\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 150 | loss: 11.48270 - acc: 0.5013 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 151  | total loss: \u001b[1m\u001b[32m11.48572\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 151 | loss: 11.48572 - acc: 0.5012 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 152  | total loss: \u001b[1m\u001b[32m11.48844\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 152 | loss: 11.48844 - acc: 0.5011 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 153  | total loss: \u001b[1m\u001b[32m11.43332\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 153 | loss: 11.43332 - acc: 0.5035 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 154  | total loss: \u001b[1m\u001b[32m11.44128\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 154 | loss: 11.44128 - acc: 0.5031 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 155  | total loss: \u001b[1m\u001b[32m11.44845\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 155 | loss: 11.44845 - acc: 0.5028 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 156  | total loss: \u001b[1m\u001b[32m11.45490\u001b[0m\u001b[0m | time: 1.110s\n","| Adam | epoch: 156 | loss: 11.45490 - acc: 0.5025 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 157  | total loss: \u001b[1m\u001b[32m11.46070\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 157 | loss: 11.46070 - acc: 0.5023 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 158  | total loss: \u001b[1m\u001b[32m11.46592\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 158 | loss: 11.46592 - acc: 0.5020 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 159  | total loss: \u001b[1m\u001b[32m11.47062\u001b[0m\u001b[0m | time: 1.110s\n","| Adam | epoch: 159 | loss: 11.47062 - acc: 0.5018 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 160  | total loss: \u001b[1m\u001b[32m11.47485\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 160 | loss: 11.47485 - acc: 0.5017 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 161  | total loss: \u001b[1m\u001b[32m11.53622\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 161 | loss: 11.53622 - acc: 0.4990 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 162  | total loss: \u001b[1m\u001b[32m11.53389\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 162 | loss: 11.53389 - acc: 0.4991 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 163  | total loss: \u001b[1m\u001b[32m11.53180\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 163 | loss: 11.53180 - acc: 0.4992 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 164  | total loss: \u001b[1m\u001b[32m11.52991\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 164 | loss: 11.52991 - acc: 0.4993 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 165  | total loss: \u001b[1m\u001b[32m11.52997\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 165 | loss: 11.52997 - acc: 0.4968 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 166  | total loss: \u001b[1m\u001b[32m11.53002\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 166 | loss: 11.53002 - acc: 0.4947 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 167  | total loss: \u001b[1m\u001b[32m11.52831\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 167 | loss: 11.52831 - acc: 0.4952 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 168  | total loss: \u001b[1m\u001b[32m11.47092\u001b[0m\u001b[0m | time: 1.121s\n","| Adam | epoch: 168 | loss: 11.47092 - acc: 0.4982 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 169  | total loss: \u001b[1m\u001b[32m11.47512\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 169 | loss: 11.47512 - acc: 0.4984 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 170  | total loss: \u001b[1m\u001b[32m11.47890\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 170 | loss: 11.47890 - acc: 0.4985 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 171  | total loss: \u001b[1m\u001b[32m11.51560\u001b[0m\u001b[0m | time: 1.109s\n","| Adam | epoch: 171 | loss: 11.51560 - acc: 0.4962 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 172  | total loss: \u001b[1m\u001b[32m11.51534\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 172 | loss: 11.51534 - acc: 0.4966 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 173  | total loss: \u001b[1m\u001b[32m11.51509\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 173 | loss: 11.51509 - acc: 0.4969 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 174  | total loss: \u001b[1m\u001b[32m11.51488\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 174 | loss: 11.51488 - acc: 0.4972 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 175  | total loss: \u001b[1m\u001b[32m11.51468\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 175 | loss: 11.51468 - acc: 0.4975 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 176  | total loss: \u001b[1m\u001b[32m11.51451\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 176 | loss: 11.51451 - acc: 0.4977 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 177  | total loss: \u001b[1m\u001b[32m11.51435\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 177 | loss: 11.51435 - acc: 0.4980 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 178  | total loss: \u001b[1m\u001b[32m11.51421\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 178 | loss: 11.51421 - acc: 0.4982 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 179  | total loss: \u001b[1m\u001b[32m11.51408\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 179 | loss: 11.51408 - acc: 0.4983 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 180  | total loss: \u001b[1m\u001b[32m11.51396\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 180 | loss: 11.51396 - acc: 0.4985 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 181  | total loss: \u001b[1m\u001b[32m11.51386\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 181 | loss: 11.51386 - acc: 0.4987 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 182  | total loss: \u001b[1m\u001b[32m11.51377\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 182 | loss: 11.51377 - acc: 0.4988 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 183  | total loss: \u001b[1m\u001b[32m11.51368\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 183 | loss: 11.51368 - acc: 0.4989 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 184  | total loss: \u001b[1m\u001b[32m11.51535\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 184 | loss: 11.51535 - acc: 0.4965 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 185  | total loss: \u001b[1m\u001b[32m11.51511\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 185 | loss: 11.51511 - acc: 0.4969 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 186  | total loss: \u001b[1m\u001b[32m11.51489\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 186 | loss: 11.51489 - acc: 0.4972 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 187  | total loss: \u001b[1m\u001b[32m11.51470\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 187 | loss: 11.51470 - acc: 0.4975 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 188  | total loss: \u001b[1m\u001b[32m11.51452\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 188 | loss: 11.51452 - acc: 0.4977 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 189  | total loss: \u001b[1m\u001b[32m11.51436\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 189 | loss: 11.51436 - acc: 0.4979 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 190  | total loss: \u001b[1m\u001b[32m11.51422\u001b[0m\u001b[0m | time: 1.123s\n","| Adam | epoch: 190 | loss: 11.51422 - acc: 0.4982 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 191  | total loss: \u001b[1m\u001b[32m11.51409\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 191 | loss: 11.51409 - acc: 0.4983 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 192  | total loss: \u001b[1m\u001b[32m11.51397\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 192 | loss: 11.51397 - acc: 0.4985 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 193  | total loss: \u001b[1m\u001b[32m11.51387\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 193 | loss: 11.51387 - acc: 0.4987 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 194  | total loss: \u001b[1m\u001b[32m11.51377\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 194 | loss: 11.51377 - acc: 0.4988 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 195  | total loss: \u001b[1m\u001b[32m11.45785\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 195 | loss: 11.45785 - acc: 0.5014 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 196  | total loss: \u001b[1m\u001b[32m11.46336\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 196 | loss: 11.46336 - acc: 0.5013 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 197  | total loss: \u001b[1m\u001b[32m11.46832\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 197 | loss: 11.46832 - acc: 0.5011 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 198  | total loss: \u001b[1m\u001b[32m11.47278\u001b[0m\u001b[0m | time: 1.124s\n","| Adam | epoch: 198 | loss: 11.47278 - acc: 0.5010 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 199  | total loss: \u001b[1m\u001b[32m11.47679\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 199 | loss: 11.47679 - acc: 0.5009 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 200  | total loss: \u001b[1m\u001b[32m11.48040\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 200 | loss: 11.48040 - acc: 0.5008 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 201  | total loss: \u001b[1m\u001b[32m11.48366\u001b[0m\u001b[0m | time: 1.121s\n","| Adam | epoch: 201 | loss: 11.48366 - acc: 0.5007 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 202  | total loss: \u001b[1m\u001b[32m11.48658\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 202 | loss: 11.48658 - acc: 0.5007 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 203  | total loss: \u001b[1m\u001b[32m11.48922\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 203 | loss: 11.48922 - acc: 0.5006 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 204  | total loss: \u001b[1m\u001b[32m11.49333\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 204 | loss: 11.49333 - acc: 0.4980 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 205  | total loss: \u001b[1m\u001b[32m11.49529\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 205 | loss: 11.49529 - acc: 0.4982 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 206  | total loss: \u001b[1m\u001b[32m11.49705\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 206 | loss: 11.49705 - acc: 0.4984 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 207  | total loss: \u001b[1m\u001b[32m11.49864\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 207 | loss: 11.49864 - acc: 0.4986 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 208  | total loss: \u001b[1m\u001b[32m11.50007\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 208 | loss: 11.50007 - acc: 0.4987 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 209  | total loss: \u001b[1m\u001b[32m11.50135\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 209 | loss: 11.50135 - acc: 0.4988 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 210  | total loss: \u001b[1m\u001b[32m11.50251\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 210 | loss: 11.50251 - acc: 0.4990 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 211  | total loss: \u001b[1m\u001b[32m11.50355\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 211 | loss: 11.50355 - acc: 0.4991 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 212  | total loss: \u001b[1m\u001b[32m11.50449\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 212 | loss: 11.50449 - acc: 0.4992 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 213  | total loss: \u001b[1m\u001b[32m11.50533\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 213 | loss: 11.50533 - acc: 0.4992 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 214  | total loss: \u001b[1m\u001b[32m11.50609\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 214 | loss: 11.50609 - acc: 0.4993 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 215  | total loss: \u001b[1m\u001b[32m11.50678\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 215 | loss: 11.50678 - acc: 0.4994 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 216  | total loss: \u001b[1m\u001b[32m11.45156\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 216 | loss: 11.45156 - acc: 0.5019 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 217  | total loss: \u001b[1m\u001b[32m11.45769\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 217 | loss: 11.45769 - acc: 0.5018 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 218  | total loss: \u001b[1m\u001b[32m11.46322\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 218 | loss: 11.46322 - acc: 0.5016 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 219  | total loss: \u001b[1m\u001b[32m11.46819\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 219 | loss: 11.46819 - acc: 0.5014 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 220  | total loss: \u001b[1m\u001b[32m11.47266\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 220 | loss: 11.47266 - acc: 0.5013 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 221  | total loss: \u001b[1m\u001b[32m11.47669\u001b[0m\u001b[0m | time: 1.124s\n","| Adam | epoch: 221 | loss: 11.47669 - acc: 0.5012 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 222  | total loss: \u001b[1m\u001b[32m11.48031\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 222 | loss: 11.48031 - acc: 0.5010 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 223  | total loss: \u001b[1m\u001b[32m11.48357\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 223 | loss: 11.48357 - acc: 0.5009 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 224  | total loss: \u001b[1m\u001b[32m11.48825\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 224 | loss: 11.48825 - acc: 0.4983 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 225  | total loss: \u001b[1m\u001b[32m11.49072\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 225 | loss: 11.49072 - acc: 0.4985 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 226  | total loss: \u001b[1m\u001b[32m11.49294\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 226 | loss: 11.49294 - acc: 0.4987 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 227  | total loss: \u001b[1m\u001b[32m11.49494\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 227 | loss: 11.49494 - acc: 0.4988 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 228  | total loss: \u001b[1m\u001b[32m11.49674\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 228 | loss: 11.49674 - acc: 0.4989 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 229  | total loss: \u001b[1m\u001b[32m11.49835\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 229 | loss: 11.49835 - acc: 0.4990 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 230  | total loss: \u001b[1m\u001b[32m11.49981\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 230 | loss: 11.49981 - acc: 0.4991 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 231  | total loss: \u001b[1m\u001b[32m11.50112\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 231 | loss: 11.50112 - acc: 0.4992 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 232  | total loss: \u001b[1m\u001b[32m11.50230\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 232 | loss: 11.50230 - acc: 0.4993 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 233  | total loss: \u001b[1m\u001b[32m11.50337\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 233 | loss: 11.50337 - acc: 0.4994 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 234  | total loss: \u001b[1m\u001b[32m11.50432\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 234 | loss: 11.50432 - acc: 0.4994 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 235  | total loss: \u001b[1m\u001b[32m11.50518\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 235 | loss: 11.50518 - acc: 0.4995 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 236  | total loss: \u001b[1m\u001b[32m11.50596\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 236 | loss: 11.50596 - acc: 0.4995 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 237  | total loss: \u001b[1m\u001b[32m11.50665\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 237 | loss: 11.50665 - acc: 0.4996 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 238  | total loss: \u001b[1m\u001b[32m11.50728\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 238 | loss: 11.50728 - acc: 0.4996 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 239  | total loss: \u001b[1m\u001b[32m11.39618\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 239 | loss: 11.39618 - acc: 0.5047 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 240  | total loss: \u001b[1m\u001b[32m11.40785\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 240 | loss: 11.40785 - acc: 0.5042 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 241  | total loss: \u001b[1m\u001b[32m11.41836\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 241 | loss: 11.41836 - acc: 0.5038 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 242  | total loss: \u001b[1m\u001b[32m11.37198\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 242 | loss: 11.37198 - acc: 0.5059 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 243  | total loss: \u001b[1m\u001b[32m11.38607\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 243 | loss: 11.38607 - acc: 0.5053 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 244  | total loss: \u001b[1m\u001b[32m11.34292\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 244 | loss: 11.34292 - acc: 0.5073 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 245  | total loss: \u001b[1m\u001b[32m11.35992\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 245 | loss: 11.35992 - acc: 0.5065 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 246  | total loss: \u001b[1m\u001b[32m11.37522\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 246 | loss: 11.37522 - acc: 0.5059 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 247  | total loss: \u001b[1m\u001b[32m11.38899\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 247 | loss: 11.38899 - acc: 0.5053 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 248  | total loss: \u001b[1m\u001b[32m11.40138\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 248 | loss: 11.40138 - acc: 0.5048 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 249  | total loss: \u001b[1m\u001b[32m11.35669\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 249 | loss: 11.35669 - acc: 0.5068 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 250  | total loss: \u001b[1m\u001b[32m11.37231\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 250 | loss: 11.37231 - acc: 0.5061 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 251  | total loss: \u001b[1m\u001b[32m11.38637\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 251 | loss: 11.38637 - acc: 0.5055 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 252  | total loss: \u001b[1m\u001b[32m11.39903\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 252 | loss: 11.39903 - acc: 0.5050 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 253  | total loss: \u001b[1m\u001b[32m11.41042\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 253 | loss: 11.41042 - acc: 0.5045 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 254  | total loss: \u001b[1m\u001b[32m11.42067\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 254 | loss: 11.42067 - acc: 0.5040 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 255  | total loss: \u001b[1m\u001b[32m11.42989\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 255 | loss: 11.42989 - acc: 0.5036 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 256  | total loss: \u001b[1m\u001b[32m11.43820\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 256 | loss: 11.43820 - acc: 0.5033 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 257  | total loss: \u001b[1m\u001b[32m11.44567\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 257 | loss: 11.44567 - acc: 0.5029 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 258  | total loss: \u001b[1m\u001b[32m11.45240\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 258 | loss: 11.45240 - acc: 0.5026 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 259  | total loss: \u001b[1m\u001b[32m11.45845\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 259 | loss: 11.45845 - acc: 0.5024 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 260  | total loss: \u001b[1m\u001b[32m11.46390\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 260 | loss: 11.46390 - acc: 0.5021 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 261  | total loss: \u001b[1m\u001b[32m11.46880\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 261 | loss: 11.46880 - acc: 0.5019 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 262  | total loss: \u001b[1m\u001b[32m11.47321\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 262 | loss: 11.47321 - acc: 0.5017 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 263  | total loss: \u001b[1m\u001b[32m11.47718\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 263 | loss: 11.47718 - acc: 0.5016 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 264  | total loss: \u001b[1m\u001b[32m11.48076\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 264 | loss: 11.48076 - acc: 0.5014 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 265  | total loss: \u001b[1m\u001b[32m11.48397\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 265 | loss: 11.48397 - acc: 0.5013 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 266  | total loss: \u001b[1m\u001b[32m11.48687\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 266 | loss: 11.48687 - acc: 0.5011 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 267  | total loss: \u001b[1m\u001b[32m11.48947\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 267 | loss: 11.48947 - acc: 0.5010 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 268  | total loss: \u001b[1m\u001b[32m11.49182\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 268 | loss: 11.49182 - acc: 0.5009 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 269  | total loss: \u001b[1m\u001b[32m11.49393\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 269 | loss: 11.49393 - acc: 0.5008 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 270  | total loss: \u001b[1m\u001b[32m11.49583\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 270 | loss: 11.49583 - acc: 0.5007 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 271  | total loss: \u001b[1m\u001b[32m11.49754\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 271 | loss: 11.49754 - acc: 0.5007 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 272  | total loss: \u001b[1m\u001b[32m11.49908\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 272 | loss: 11.49908 - acc: 0.5006 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 273  | total loss: \u001b[1m\u001b[32m11.50046\u001b[0m\u001b[0m | time: 1.109s\n","| Adam | epoch: 273 | loss: 11.50046 - acc: 0.5005 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 274  | total loss: \u001b[1m\u001b[32m11.44761\u001b[0m\u001b[0m | time: 1.122s\n","| Adam | epoch: 274 | loss: 11.44761 - acc: 0.5005 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 275  | total loss: \u001b[1m\u001b[32m11.45414\u001b[0m\u001b[0m | time: 1.121s\n","| Adam | epoch: 275 | loss: 11.45414 - acc: 0.5004 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 276  | total loss: \u001b[1m\u001b[32m11.46002\u001b[0m\u001b[0m | time: 1.120s\n","| Adam | epoch: 276 | loss: 11.46002 - acc: 0.5004 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 277  | total loss: \u001b[1m\u001b[32m11.46531\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 277 | loss: 11.46531 - acc: 0.5004 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 278  | total loss: \u001b[1m\u001b[32m11.47007\u001b[0m\u001b[0m | time: 1.118s\n","| Adam | epoch: 278 | loss: 11.47007 - acc: 0.5003 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 279  | total loss: \u001b[1m\u001b[32m11.41849\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 279 | loss: 11.41849 - acc: 0.5028 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 280  | total loss: \u001b[1m\u001b[32m11.42971\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 280 | loss: 11.42971 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 281  | total loss: \u001b[1m\u001b[32m11.43803\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 281 | loss: 11.43803 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 282  | total loss: \u001b[1m\u001b[32m11.44552\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 282 | loss: 11.44552 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 283  | total loss: \u001b[1m\u001b[32m11.45403\u001b[0m\u001b[0m | time: 1.113s\n","| Adam | epoch: 283 | loss: 11.45403 - acc: 0.4975 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 284  | total loss: \u001b[1m\u001b[32m11.45992\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 284 | loss: 11.45992 - acc: 0.4978 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 285  | total loss: \u001b[1m\u001b[32m11.46522\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 285 | loss: 11.46522 - acc: 0.4980 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 286  | total loss: \u001b[1m\u001b[32m11.46999\u001b[0m\u001b[0m | time: 1.124s\n","| Adam | epoch: 286 | loss: 11.46999 - acc: 0.4982 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 287  | total loss: \u001b[1m\u001b[32m11.47606\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 287 | loss: 11.47606 - acc: 0.4959 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 288  | total loss: \u001b[1m\u001b[32m11.47975\u001b[0m\u001b[0m | time: 1.109s\n","| Adam | epoch: 288 | loss: 11.47975 - acc: 0.4963 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 289  | total loss: \u001b[1m\u001b[32m11.48307\u001b[0m\u001b[0m | time: 1.111s\n","| Adam | epoch: 289 | loss: 11.48307 - acc: 0.4967 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 290  | total loss: \u001b[1m\u001b[32m11.48605\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 290 | loss: 11.48605 - acc: 0.4970 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 291  | total loss: \u001b[1m\u001b[32m11.48874\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 291 | loss: 11.48874 - acc: 0.4973 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 292  | total loss: \u001b[1m\u001b[32m11.43529\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 292 | loss: 11.43529 - acc: 0.5001 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 293  | total loss: \u001b[1m\u001b[32m11.44306\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 293 | loss: 11.44306 - acc: 0.5001 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 294  | total loss: \u001b[1m\u001b[32m11.45004\u001b[0m\u001b[0m | time: 1.112s\n","| Adam | epoch: 294 | loss: 11.45004 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 295  | total loss: \u001b[1m\u001b[32m11.45633\u001b[0m\u001b[0m | time: 1.116s\n","| Adam | epoch: 295 | loss: 11.45633 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 296  | total loss: \u001b[1m\u001b[32m11.40613\u001b[0m\u001b[0m | time: 1.123s\n","| Adam | epoch: 296 | loss: 11.40613 - acc: 0.5025 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 297  | total loss: \u001b[1m\u001b[32m11.41681\u001b[0m\u001b[0m | time: 1.115s\n","| Adam | epoch: 297 | loss: 11.41681 - acc: 0.5023 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 298  | total loss: \u001b[1m\u001b[32m11.42642\u001b[0m\u001b[0m | time: 1.117s\n","| Adam | epoch: 298 | loss: 11.42642 - acc: 0.5021 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 299  | total loss: \u001b[1m\u001b[32m11.43507\u001b[0m\u001b[0m | time: 1.114s\n","| Adam | epoch: 299 | loss: 11.43507 - acc: 0.5019 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n","Training Step: 300  | total loss: \u001b[1m\u001b[32m11.44285\u001b[0m\u001b[0m | time: 1.119s\n","| Adam | epoch: 300 | loss: 11.44285 - acc: 0.5017 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n","--\n"]}]},{"cell_type":"code","source":["history"],"metadata":{"id":"0DYR-qG8aJff","executionInfo":{"status":"ok","timestamp":1650012825445,"user_tz":-180,"elapsed":513,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"jjEO-3KUusu6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650012814590,"user_tz":-180,"elapsed":13,"user":{"displayName":"Dr SANA B","userId":"15210487070757570898"}},"outputId":"c1ed3e6d-b302-42f7-b959-6e6d5818f81b"},"source":["model.save('/content/drive/MyDrive/PETDB/{}'.format(MODEL_NAME))"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:/content/drive/MyDrive/PETDB/pet_classifier_2_Conv_basic is not in all_model_checkpoint_paths. Manually adding it.\n"]}]}]}